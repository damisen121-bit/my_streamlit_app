import streamlit as st
import matplotlib.pyplot as plt
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from collections import Counter
from janome.tokenizer import Tokenizer
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans



import pandas as pd

# Google Sheets èªè¨¼
scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
creds = ServiceAccountCredentials.from_json_keyfile_name("stremlit-voting-dfc8b6ac90cc.json", scope)
client = gspread.authorize(creds)

# ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’é–‹ã
sheet = client.open("PreferenceVotes").sheet1

# é¸æŠè‚¢
options = ["Option A", "Option B", "Option C"]

st.title("ğŸ—³ï¸ Multi-user Voting with Google Sheets")
#æŠ•ç¥¨è€…ã®åå‰ã¨ã‚³ãƒ¡ãƒ³ãƒˆ
name = st.text_input("Your name or nickname")
comment = st.text_area("Why did you rank this way?")


ranked = st.multiselect("Rank the options (top = highest)", options, default=options)

if st.button("Submit"):
    if len(ranked) == len(options):
        sheet.append_row([name, comment] + ranked)
        st.success("Your vote has been saved to Google Sheets!")
    else:
        st.warning("Please rank all options before submitting.")


data = sheet.get_all_records()
df = pd.DataFrame(data)



# ã‚¹ã‚³ã‚¢é›†è¨ˆ
scores = {opt: 0 for opt in options}
for _, row in df.iterrows():
    for i, opt in enumerate([row["Rank1"], row["Rank2"], row["Rank3"]]):
        scores[opt] += len(options) - i - 1

score_df = pd.DataFrame(scores.items(), columns=["Option", "Score"]).sort_values("Score", ascending=False)
st.subheader("ğŸ“Š Aggregated Results")
st.bar_chart(score_df.set_index("Option"))
st.subheader("ğŸ—£ï¸ Voter Comments")
for _, row in df.iterrows():
    st.markdown(f"**{row['Name']}**: {row['Comment']}")

# --- ã‚³ãƒ¡ãƒ³ãƒˆã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º ---
if not df.empty and "Comment" in df.columns:
    comments = df["Comment"].dropna().tolist()

    tokenizer = Tokenizer()
    words = []
    for comment in comments:
        tokens = tokenizer.tokenize(comment)
        for token in tokens:
            # åè©ã ã‘ã‚’æŠ½å‡º
            if token.part_of_speech.startswith("åè©"):
                words.append(token.surface)
# é »åº¦é›†è¨ˆ
    word_counts = Counter(words)
    common_words = word_counts.most_common(10)

    if common_words:
        keywords_df = pd.DataFrame(common_words, columns=["Keyword", "Count"])
        st.subheader("ğŸ”‘ ã‚³ãƒ¡ãƒ³ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é »åº¦ï¼ˆä¸Šä½10ä»¶ï¼‰")
        st.bar_chart(keywords_df.set_index("Keyword"))
    else:
        st.info("ã¾ã ã‚³ãƒ¡ãƒ³ãƒˆãŒå°‘ãªã„ãŸã‚ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã¯ã§ãã¾ã›ã‚“ã€‚")

# --- ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆ ---
if not df.empty and "Comment" in df.columns:
    comments = df["Comment"].dropna().tolist()
    text = " ".join(comments)

    # WordCloudç”Ÿæˆï¼ˆæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’æŒ‡å®šï¼‰
    wordcloud = WordCloud(
        font_path="C:/Windows/Fonts/msgothic.ttc",  # ç’°å¢ƒã«åˆã‚ã›ã¦å¤‰æ›´
        width=800,
        height=400,
        background_color="white"
    ).generate(text)

    # Streamlitã§è¡¨ç¤º
    st.subheader("â˜ï¸ CommentCloud")
    fig, ax = plt.subplots()
    ax.imshow(wordcloud, interpolation="bilinear")
    ax.axis("off")
    st.pyplot(fig)

# --- ã‚³ãƒ¡ãƒ³ãƒˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° ---
if not df.empty and "Comment" in df.columns:
    comments = df["Comment"].dropna().tolist()

    if len(comments) < 3:
        st.info("ã‚³ãƒ¡ãƒ³ãƒˆãŒå°‘ãªã„ãŸã‚ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
        for c in comments:
            st.markdown(f"- {c}")
    else:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.cluster import KMeans
        from janome.tokenizer import Tokenizer
    
    tokenizer = Tokenizer()
    def tokenize(text):
        return [token.surface for token in tokenizer.tokenize(text) if token.part_of_speech.startswith("åè©")]

    
    vectorizer = TfidfVectorizer(tokenizer=tokenize)
    X = vectorizer.fit_transform(comments)

    
    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X)

    
    cluster_df = pd.DataFrame({"Comment": comments, "Cluster": labels})

    st.subheader("ğŸ§© ã‚³ãƒ¡ãƒ³ãƒˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ")
    for cluster_id in sorted(cluster_df["Cluster"].unique()):
        st.markdown(f"### ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}")
        for comment in cluster_df[cluster_df["Cluster"] == cluster_id]["Comment"]:
            st.markdown(f"- {comment}")





if st.button("ğŸ”„ Reset all votes"):
    sheet.resize(rows=1)
    st.warning("All votes have been cleared.")